
% REFERENCES

@article{jacob2020cross,
  title={Cross-fitting and averaging for machine learning estimation of heterogeneous treatment effects},
  author={Jacob, Daniel},
  journal={arXiv preprint arXiv:2007.02852},
  year={2020}
}

@article{wright1921correlation,
  title={Correlation and causation},
  author={Wright, Sewall},
  journal={Journal of agricultural research},
  volume={20},
  number={7},
  pages={557},
  year={1921}
}

@inbook{Pearl2004,
   abstract = {This chapter surveys the development of graphical models known as Bayesian networks, summarizes their semantical basis, and assesses their properties and applications to reasoning and planning. Bayesian networks are directed acyclic graphs (DAGs) in which the nodes represent variables of interest (e.g., the temperature of a device, the gender of a patient, a feature of an object, the occurrence of an event) and the links represent causal influences among the variables. The strength of an influence is represented by conditional probabilities that are attached to each cluster of parents-child nodes in the network.},
   author = {Judea Pearl},
   doi = {10.1201/b16812-50},
   journal = {Computer Science Handbook, Second Edition},
   title = {Graphical models for probabilistic and causal reasoning},
   year = {2004},
}

@article{Blackwell2007,
   abstract = {},
   author = {David Blackwell and James B. MacQueen},
   doi = {10.1214/aos/1176342372},
   issue = {2},
   journal = {The Annals of Statistics},
   title = {Ferguson Distributions Via Polya Urn Schemes},
   volume = {1},
   year = {2007},
}

@article{Rubin1974,
   abstract = {Presents a discussion of matching, randomization, random sampling, and other methods of controlling extraneous variation. The objective was to specify the benefits of randomization in estimating causal effects of treatments. It is concluded that randomization should be employed whenever possible but that the use of carefully controlled nonrandomized data to estimate causal effects is a reasonable and necessary procedure in many cases. (15 ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved). © 1974 American Psychological Association.},
   author = {Donald B. Rubin},
   doi = {10.1037/h0037350},
   issn = {00220663},
   issue = {5},
   journal = {Journal of Educational Psychology},
   title = {Estimating causal effects of treatments in randomized and nonrandomized studies},
   volume = {66},
   year = {1974},
}

@article{Haavelmo1943,
   abstract = {Measurement of parameters occurring in theoretical equation systems is one of the most important problems of econometrics. If our equations were exact in the observable economic variables involved, this problem would not be one of statistics, but a purely mathematical one of solving a certain system of" observational" equations, having the parameters in question as unknowns. This might itself present complicated and interesting problems, such as the problem of whether or not there is a one-to-one correspondence between each},
   author = {Trygve Haavelmo},
   doi = {10.2307/1905714},
   issn = {00129682},
   issue = {1},
   journal = {Econometrica},
   title = {The Statistical Implications of a System of Simultaneous Equations},
   volume = {11},
   year = {1943},
}

@article{chen2024bayesian,
  title={A Bayesian machine learning approach for estimating heterogeneous survivor causal effects: Applications to a critical care trial},
  author={Chen, Xinyuan and Harhay, Michael O and Tong, Guangyu and Li, Fan},
  journal={The Annals of Applied Statistics},
  volume={18},
  number={1},
  pages={350--374},
  year={2024},
  publisher={Institute of Mathematical Statistics},
  doi={10.1214/23-AOAS1792}
}

@article{USFDA2021,
   abstract = {},
   author = {USFDA and U.S. Food and Food and Drug Administration},
   issue = {February},
   journal = {February 2021},
   title = {COVID-19: Developing Drugs and Biological Products for Treatment or Prevention Guidance for Industry},
   year = {2021},
}

@article{Funk2011,
   abstract = {},
   author = {Michele Jonsson Funk and Daniel Westreich and Chris Wiesen and Til Stürmer and M. Alan Brookhart and Marie Davidian},
   doi = {10.1093/aje/kwq439},
   issn = {00029262},
   issue = {7},
   journal = {American Journal of Epidemiology},
   title = {Doubly robust estimation of causal effects},
   volume = {173},
   year = {2011},
}

@article{Blei2012,
   abstract = {For concreteness, consider the problem of modeling response time (RT) distributions. Psychol-ogists believe that several cognitive processes contribute to producing behavioral responses (Luce, 1986), and therefore it is a scientifically relevant question how to decompose observed RTs into their underlying components. The generative model we describe below expresses one possible process by which latent causes (e.g., cognitive processes) might give rise to observed data (e.g., RTs). 2 Using Bayes' rule, we can invert the generative model to recover a distribution over the possible set of latent causes of our observations. The inferred latent causes are commonly known as " clusters. " 2.1 Finite mixture modeling One approach to this problem is finite mixture modeling. A finite mixture model assumes that there are K clusters, each associated with a parameter θ k . Each observation y n is assumed to be generated by first choosing a cluster c n according to P (c n) and then generating the observation from its corresponding observation distribution parameterized by θ cn . In the RT modeling problem, each observation is a scalar RT and each cluster specifies a hypothetical distribution F (y n |θ cn) over the observed RT. 3 Finite mixtures can accommodate many kinds of data by changing the data generating distri-bution. For example, in a Gaussian mixture model the data—conditioned on knowing their cluster assignments—are assumed to be drawn from a Gaussian distribution. The cluster parameters θ k are the means of the components (assuming known variances). Figure 1 illustrates data drawn from a Gaussian mixture with four clusters. Bayesian mixture models further contain a prior over the mixing distribution P (c), and a prior over the cluster parameters: θ ∼ G 0 . (We denote the prior over cluster parameters G 0 to later make a connection to BNP mixture models.) In a Gaussian mixture, for example, it is computationally convenient to choose the cluster parameter prior to be Gaussian. A convenient choice for the distribution on the mixing distribution is a Dirichlet. We will build on fully Bayesian mixture modeling when we discuss Bayesian nonparametric mixture models. This generative process defines a joint distribution over the observations, cluster assignments, and cluster parameters, P (y, c, θ) =},
   author = {Samuel J. Gershman and David M. Blei},
   issue = {1},
   journal = {Journal of Mathematical Psychology},
   title = {A Tutorial on Bayesian Nonparametric Models},
   volume = {56},
   year = {2012},
}

@article{Oganisian2019ChiRP,
    journal = {Journal of Open Source Software},
    doi = {10.21105/joss.01287},
    issn = {2475-9066},
    number = {35},
    publisher = {The Open Journal},
    title = {ChiRP: Chinese Restaurant Process Mixtures for Regression and Clustering},
    url = {http://dx.doi.org/10.21105/joss.01287},
    volume = {4},
    author = {Oganisian, Arman},
    pages = {1287},
    date = {2019-03-26},
    year = {2019},
    month = {3},
    day = {26},
}


@article{Li2019,
   abstract = {Bayesian nonparametric (BNP) models are becoming increasingly important in psychology, both as theoretical models of cognition and as analytic tools. However, existing tutorials tend to be at a level of abstraction largely impenetrable by non-technicians. This tutorial aims to help beginners understand key concepts by working through important but often omitted derivations carefully and explicitly, with a focus on linking the mathematics with a practical computation solution for a Dirichlet Process Mixture Model (DPMM)—one of the most widely used BNP methods. Abstract concepts are made explicit and concrete to non-technical readers by working through the theory that gives rise to them. A publicly accessible computer program written in the statistical language R is explained line-by-line to help readers understand the computation algorithm. The algorithm is also linked to a construction method called the Chinese Restaurant Process in an accessible tutorial in this journal (Gershman and Blei, 2012). The overall goals are to help readers understand more fully the theory and application so that they may apply BNP methods in their own work and leverage the technical details in this tutorial to develop novel methods.},
   author = {Yuelin Li and Elizabeth Schofield and Mithat Gönen},
   doi = {10.1016/j.jmp.2019.04.004},
   issn = {10960880},
   journal = {Journal of Mathematical Psychology},
   title = {A tutorial on Dirichlet process mixture modeling},
   volume = {91},
   year = {2019},
}
@article{Ryan2020,
   abstract = {Background: Bayesian adaptive methods are increasingly being used to design clinical trials and offer several advantages over traditional approaches. Decisions at analysis points are usually based on the posterior distribution of the treatment effect. However, there is some confusion as to whether control of type I error is required for Bayesian designs as this is a frequentist concept. Methods: We discuss the arguments for and against adjusting for multiplicities in Bayesian trials with interim analyses. With two case studies we illustrate the effect of including interim analyses on type I/II error rates in Bayesian clinical trials where no adjustments for multiplicities are made. We propose several approaches to control type I error, and also alternative methods for decision-making in Bayesian clinical trials. Results: In both case studies we demonstrated that the type I error was inflated in the Bayesian adaptive designs through incorporation of interim analyses that allowed early stopping for efficacy and without adjustments to account for multiplicity. Incorporation of early stopping for efficacy also increased the power in some instances. An increase in the number of interim analyses that only allowed early stopping for futility decreased the type I error, but also decreased power. An increase in the number of interim analyses that allowed for either early stopping for efficacy or futility generally increased type I error and decreased power. Conclusions: Currently, regulators require demonstration of control of type I error for both frequentist and Bayesian adaptive designs, particularly for late-phase trials. To demonstrate control of type I error in Bayesian adaptive designs, adjustments to the stopping boundaries are usually required for designs that allow for early stopping for efficacy as the number of analyses increase. If the designs only allow for early stopping for futility then adjustments to the stopping boundaries are not needed to control type I error. If one instead uses a strict Bayesian approach, which is currently more accepted in the design and analysis of exploratory trials, then type I errors could be ignored and the designs could instead focus on the posterior probabilities of treatment effects of clinically-relevant values.},
   author = {Elizabeth G. Ryan and Kristian Brock and Simon Gates and Daniel Slade},
   doi = {10.1186/s12874-020-01042-7},
   issn = {14712288},
   issue = {1},
   journal = {BMC Medical Research Methodology},
   title = {Do we need to adjust for interim analyses in a Bayesian adaptive trial design?},
   volume = {20},
   year = {2020},
}
@article{Tsiatis2004,
   abstract = {A class of semiparametric estimators are proposed in the general setting of functional measurement error models. The estimators follow from estimating equations that are based on the semiparametric efficient score derived under a possibly incorrect distributional assumption for the unobserved 'measured with error' covariates. It is shown that such estimators are consistent and asymptotically normal even with misspecification and are efficient if computed under the truth. The methods are demonstrated with a simulation study of a quadratic logistic regression model with measurement error. © 2004 Biometrika Trust.},
   author = {Anastasios A. Tsiatis and Yanyuan Ma},
   doi = {10.1093/biomet/91.4.835},
   issn = {00063444},
   issue = {4},
   journal = {Biometrika},
   title = {Locally efficient semiparametric estimators for functional measurement error models},
   volume = {91},
   year = {2004},
}
@article{Hahn2020,
   abstract = {This paper presents a novel nonlinear regression model for estimating heterogeneous treatment effects, geared specifically towards situations with small effect sizes, heterogeneous effects, and strong confounding by observables. Standard nonlinear regression models, which may work quite well for prediction, have two notable weaknesses when used to estimate heterogeneous treatment effects. First, they can yield badly biased estimates of treatment effects when fit to data with strong confounding. The Bayesian causal forest model presented in this paper avoids this problem by directly incorporating an estimate of the propensity function in the specification of the response model, implicitly inducing a covariatedependent prior on the regression function. Second, standard approaches to response surface modeling do not provide adequate control over the strength of regularization over effect heterogeneity. The Bayesian causal forest model permits treatment effect heterogeneity to be regularized separately from the prognostic effect of control variables, making it possible to informatively "shrink to homogeneity". While we focus on observational data, our methods are equally useful for inferring heterogeneous treatment effects from randomized controlled experiments where careful regularization is somewhat less complicated but no less important. We illustrate these benefits via the reanalysis of an observational study assessing the causal effects of smoking on medical expenditures as well as extensive simulation studies.},
   author = {P. Richard Hahn and Jared S. Murray and Carlos M. Carvalho},
   doi = {10.1214/19-BA1195},
   issn = {19316690},
   issue = {3},
   journal = {Bayesian Analysis},
   title = {Bayesian regression tree models for causal inference: Regularization, confounding, and heterogeneous effects (with discussion)},
   volume = {15},
   year = {2020},
}
@article{Blette2023,
   abstract = {The currently recommended dose of dexamethasone for patients with severe or critical COVID-19 is 6 mg per day (mg/d) regardless of patient features and variation. However, patients with severe or critical COVID-19 are heterogenous in many ways (e.g., age, weight, comorbidities, disease severity, and immune features). Thus, it is conceivable that a standardized dosing protocol may not be optimal. We assessed treatment effect heterogeneity in the COVID STEROID 2 trial, which compared 6 mg/d to 12 mg/d, using a causal inference framework with Bayesian Additive Regression Trees, a flexible modeling method that detects interactive effects and nonlinear relationships among multiple patient characteristics simultaneously. We found that 12 mg/d of dexamethasone, relative to 6 mg/d, was probably associated with better long-term outcomes (days alive without life support and mortality after 90 days) among the entire trial population (i.e., no signals of harm), and probably more beneficial among those without diabetes mellitus, that were older, were not using IL-6 inhibitors at baseline, weighed less, or had higher level respiratory support at baseline. This adds more evidence supporting the use of 12 mg/d in practice for most patients not receiving other immunosuppressants and that additional study of dosing could potentially optimize clinical outcomes.},
   author = {Bryan S. Blette and Anders Granholm and Fan Li and Manu Shankar-Hari and Theis Lange and Marie Warrer Munch and Morten Hylander Møller and Anders Perner and Michael O. Harhay},
   doi = {10.1038/s41598-023-33425-3},
   issn = {20452322},
   issue = {1},
   journal = {Scientific Reports},
   title = {Causal Bayesian machine learning to assess treatment effect heterogeneity by dexamethasone dose for patients with COVID-19 and severe hypoxemia},
   volume = {13},
   year = {2023},
}
@article{Robins1994,
   abstract = {In applied problems it is common to specify a model for the conditional mean of a response given a set of regressors. A subset of the regressors may be missing for some study subjects either by design or happenstance. In this article we propose a new class of semiparametric estimators, based on inverse probability weighted estimating equations, that are consistent for parameter vector α0 of the conditional mean model when the data are missing at random in the sense of Rubin and the missingness probabilities are either known or can be parametrically modeled. We show that the asymptotic variance of the optimal estimator in our class attains the semiparametric variance bound for the model by first showing that our estimation problem is a special case of the general problem of parameter estimation in an arbitrary semiparametric model in which the data are missing at random and the probability of observing complete data is bounded away from 0, and then deriving a representation for the efficient score, the semiparametric variance bound, and the influence function of any regular, asymptotically linear estimator in this more general estimation problem. Because the optimal estimator depends on the unknown probability law generating the data, we propose locally and globally adaptive semiparametric efficient estimators. We compare estimators in our class with previously proposed estimators. We show that each previous estimator is asymptotically equivalent to some, usually inefficient, estimator in our class. This equivalence is a consequence of a proposition stating that every regular asymptotic linear estimator of α0 is asymptotically equivalent to some estimator in our class. We compare various estimators in a small simulation study and offer some practical recommendations. © 1994 Taylor & Francis Group, LLC.},
   author = {James M. Robins and Andrea Rotnitzky and Lue Ping Zhao},
   doi = {10.1080/01621459.1994.10476818},
   issn = {1537274X},
   issue = {427},
   journal = {Journal of the American Statistical Association},
   title = {Estimation of regression coefficients when some regressors are not always observed},
   volume = {89},
   year = {1994},
}
@misc{Li2023,
   abstract = {This paper provides a critical review of the Bayesian perspective of causal inference based on the potential outcomes framework. We review the causal estimands, assignment mechanism, the general structure of Bayesian inference of causal effects and sensitivity analysis. We highlight issues that are unique to Bayesian causal inference, including the role of the propensity score, the definition of identifiability, the choice of priors in both low- and high-dimensional regimes. We point out the central role of covariate overlap and more generally the design stage in Bayesian causal inference. We extend the discussion to two complex assignment mechanisms: instrumental variable and time-varying treatments. We identify the strengths and weaknesses of the Bayesian approach to causal inference. Throughout, we illustrate the key concepts via examples. This article is part of the theme issue 'Bayesian inference: challenges, perspectives, and prospects'.},
   author = {Fan Li and Peng Ding and Fabrizia Mealli},
   doi = {10.1098/rsta.2022.0153},
   issn = {1364503X},
   issue = {2247},
   journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
   title = {Bayesian causal inference: a critical review},
   volume = {381},
   year = {2023},
}
@article{Oganisian2021,
   abstract = {Substantial advances in Bayesian methods for causal inference have been made in recent years. We provide an introduction to Bayesian inference for causal effects for practicing statisticians who have some familiarity with Bayesian models and would like an overview of what it can add to causal estimation in practical settings. In the paper, we demonstrate how priors can induce shrinkage and sparsity in parametric models and be used to perform probabilistic sensitivity analyses around causal assumptions. We provide an overview of nonparametric Bayesian estimation and survey their applications in the causal inference literature. Inference in the point-treatment and time-varying treatment settings are considered. For the latter, we explore both static and dynamic treatment regimes. Throughout, we illustrate implementation using off-the-shelf open source software. We hope to leave the reader with implementation-level knowledge of Bayesian causal inference using both parametric and nonparametric models. All synthetic examples and code used in the paper are publicly available on a companion GitHub repository.},
   author = {Arman Oganisian and Jason A. Roy},
   doi = {10.1002/sim.8761},
   issn = {10970258},
   issue = {2},
   journal = {Statistics in Medicine},
   title = {A practical introduction to Bayesian estimation of causal effects: Parametric and nonparametric approaches},
   volume = {40},
   year = {2021},
}
@article{Roy2018,
   abstract = {We propose a general Bayesian nonparametric (BNP) approach to causal inference in the point treatment setting. The joint distribution of the observed data (outcome, treatment, and confounders) is modeled using an enriched Dirichlet process. The combination of the observed data model and causal assumptions allows us to identify any type of causal effect—differences, ratios, or quantile effects, either marginally or for subpopulations of interest. The proposed BNP model is well-suited for causal inference problems, as it does not require parametric assumptions about the distribution of confounders and naturally leads to a computationally efficient Gibbs sampling algorithm. By flexibly modeling the joint distribution, we are also able to impute (via data augmentation) values for missing covariates within the algorithm under an assumption of ignorable missingness, obviating the need to create separate imputed data sets. This approach for imputing the missing covariates has the additional advantage of guaranteeing congeniality between the imputation model and the analysis model, and because we use a BNP approach, parametric models are avoided for imputation. The performance of the method is assessed using simulation studies. The method is applied to data from a cohort study of human immunodeficiency virus/hepatitis C virus co-infected patients.},
   author = {Jason Roy and Kirsten J. Lum and Bret Zeldow and Jordan D. Dworkin and Vincent Lo Re and Michael J. Daniels},
   doi = {10.1111/biom.12875},
   issn = {15410420},
   issue = {4},
   journal = {Biometrics},
   title = {Bayesian nonparametric generative models for causal inference with missing at random covariates},
   volume = {74},
   year = {2018},
}

@article{Berchialla2021,
   abstract = {Background: In a randomized controlled trial (RCT) with binary outcome the estimate of the marginal treatment effect can be biased by prognostic baseline covariates adjustment. Methods that target the marginal odds ratio, allowing for improved precision and power, have been devel-oped. Methods: The performance of different estimators for the treatment effect in the frequentist (targeted maximum likelihood estimator, inverse-probability-of-treatment weighting, parametric G-computation, and the semiparametric locally efficient estimator) and Bayesian (model averaging), adjustment for confounding, and generalized Bayesian causal effect estimation frameworks are assessed and compared in a simulation study under different scenarios. The use of these estimators is illustrated on an RCT in type II diabetes. Results: Model mis-specification does not increase the bias. The approaches that are not doubly robust have increased standard error (SE) under the scenario of mis-specification of the treatment model. The Bayesian estimators showed a higher type II error than frequentist estimators if noisy covariates are included in the treatment model. Conclusions: Adjusting for prognostic baseline covariates in the analysis of RCTs can have more power than intention-to-treat based tests. However, for some classes of model, when the regression model is mis-specified, inflated type I error and potential bias on treatment effect estimate may arise.},
   author = {Paola Berchialla and Veronica Sciannameo and Sara Urru and Corrado Lanera and Danila Azzolina and Dario Gregori and Ileana Baldi},
   doi = {10.3390/ijerph18157758},
   issn = {16604601},
   issue = {15},
   journal = {International Journal of Environmental Research and Public Health},
   title = {Adjustment for baseline covariates to increase efficiency in RCTs with binary endpoint: A comparison of bayesian and frequentist approaches},
   volume = {18},
   year = {2021},
}

@article{Benkeser2021,
   abstract = {Time is of the essence in evaluating potential drugs and biologics for the treatment and prevention of COVID-19. There are currently 876 randomized clinical trials (phase 2 and 3) of treatments for COVID-19 registered on clinicaltrials.gov. Covariate adjustment is a statistical analysis method with potential to improve precision and reduce the required sample size for a substantial number of these trials. Though covariate adjustment is recommended by the U.S. Food and Drug Administration and the European Medicines Agency, it is underutilized, especially for the types of outcomes (binary, ordinal, and time-to-event) that are common in COVID-19 trials. To demonstrate the potential value added by covariate adjustment in this context, we simulated two-arm, randomized trials comparing a hypothetical COVID-19 treatment versus standard of care, where the primary outcome is binary, ordinal, or time-to-event. Our simulated distributions are derived from two sources: longitudinal data on over 500 patients hospitalized at Weill Cornell Medicine New York Presbyterian Hospital and a Centers for Disease Control and Prevention preliminary description of 2449 cases. In simulated trials with sample sizes ranging from 100 to 1000 participants, we found substantial precision gains from using covariate adjustment-equivalent to 4-18% reductions in the required sample size to achieve a desired power. This was the case for a variety of estimands (targets of inference). From these simulations, we conclude that covariate adjustment is a low-risk, high-reward approach to streamlining COVID-19 treatment trials. We provide an R package and practical recommendations for implementation.},
   author = {David Benkeser and Iván Díaz and Alex Luedtke and Jodi Segal and Daniel Scharfstein and Michael Rosenblum},
   doi = {10.1111/biom.13377},
   issn = {15410420},
   issue = {4},
   journal = {Biometrics},
   title = {Improving precision and power in randomized trials for COVID-19 treatments using covariate adjustment, for binary, ordinal, and time-to-event outcomes},
   volume = {77},
   year = {2021},
}

@article{Lammers2023,
   abstract = {Importance: Frequentist statistical approaches are the most common strategies for clinical trial design; however, bayesian trial design may provide a more optimal study technique for trauma-related studies. Objective: To describe the outcomes of bayesian statistical approaches using data from the Pragmatic Randomized Optimal Platelet and Plasma Ratios (PROPPR) Trial. Design, Setting, and Participants: This quality improvement study performed a post hoc bayesian analysis of the PROPPR Trial using multiple hierarchical models to assess the association of resuscitation strategy with mortality. The PROPPR Trial took place at 12 US level I trauma centers from August 2012 to December 2013. A total of 680 severely injured trauma patients who were anticipated to require large volume transfusions were included in the study. Data analysis for this quality improvement study was conducted from December 2021 and June 2022. Interventions: In the PROPPR Trial, patients were randomized to receive a balanced transfusion (equal portions of plasma, platelets, and red blood cells [1:1:1]) vs a red blood cell-heavy strategy (1:1:2) during their initial resuscitation. Main Outcomes and Measures: Primary outcomes from the PROPPR trial included 24-hour and 30-day all-cause mortality using frequentist statistical methods. Bayesian methods were used to define the posterior probabilities associated with the resuscitation strategies at each of the original primary end points. Results: Overall, 680 patients (546 [80.3%] male; median [IQR] age, 34 [24-51] years, 330 [48.5%] with penetrating injury; median [IQR] Injury Severity Score, 26 [17-41]; 591 [87.0%] with severe hemorrhage) were included in the original PROPPR Trial. Between the groups, no significant differences in mortality were originally detected at 24 hours (12.7% vs 17.0%; adjusted risk ratio [RR], 0.75 [95% CI, 0.52-1.08]; P =.12) or 30 days (22.4% vs 26.1%; adjusted RR, 0.86 [95% CI, 0.65-1.12]; P =.26). Using bayesian approaches, a 1:1:1 resuscitation was found to have a 93% (Bayes factor, 13.7; RR, 0.75 [95% credible interval, 0.45-1.11]) and 87% (Bayes factor, 6.56; RR, 0.82 [95% credible interval, 0.57-1.16]) probability of being superior to a 1:1:2 resuscitation with regards to 24-hour and 30-day mortality, respectively. Conclusions and Relevance: In this quality improvement study, a post hoc bayesian analysis of the PROPPR Trial found evidence in support of mortality reduction with a balanced resuscitation strategy for patients in hemorrhagic shock. Bayesian statistical methods offer probability-based results capable of direct comparison between various interventions and should be considered for future studies assessing trauma-related outcomes..},
   author = {Daniel Lammers and Joshua Richman and John B. Holcomb and Jan O. Jansen},
   doi = {10.1001/jamanetworkopen.2023.0421},
   issn = {25743805},
   issue = {2},
   journal = {JAMA Network Open},
   title = {Use of Bayesian Statistics to Reanalyze Data from the Pragmatic Randomized Optimal Platelet and Plasma Ratios Trial},
   volume = {6},
   year = {2023},
}

@article{Gnecco2022,
   abstract = {This paper introduces an innovative Bayesian machine learning algorithm to draw interpretable inference on heterogeneous causal effects in the presence of imperfect compliance (e.g., under an irregular assignment mechanism). We show, through Monte Carlo simulations, that the proposed Bayesian Causal Forest with Instrumental Variable (BCF-IV) methodology outperforms other machine learning techniques tailored for causal inference in discovering and estimating the heterogeneous causal effects while controlling for the familywise error rate (or, less stringently, for the false discovery rate) at leaves’ level. BCF-IV sheds a light on the heterogeneity of causal effects in instrumental variable scenarios and, in turn, provides the policy-makers with a relevant tool for targeted policies. Its empirical application evaluates the effects of additional funding on students’ performances. The results indicate that BCF-IV could be used to enhance the effectiveness of school funding on students’ performance.},
   author = {Falco J. Bargagli-Stoffi and Kristof DE WITTE and Giorgio Gnecco},
   doi = {10.1214/21-AOAS1579},
   issn = {19417330},
   issue = {3},
   journal = {Annals of Applied Statistics},
   title = {Heterogeneous causal effects with imperfect compliance: a Bayesian machine learning approach},
   volume = {16},
   year = {2022},
}

@article{Hariton2018,
   author = {Eduardo Hariton and Joseph J. Locascio},
   doi = {10.1111/1471-0528.15199},
   issn = {14710528},
   issue = {13},
   journal = {BJOG: An International Journal of Obstetrics and Gynaecology},
   title = {Randomised controlled trials – the gold standard for effectiveness research: Study design: randomised controlled trials},
   volume = {125},
   year = {2018},
}
@article{Williams2022,
   abstract = {The rapid finding of effective therapeutics requires efficient use of available resources in clinical trials. Covariate adjustment can yield statistical estimates with improved precision, resulting in a reduction in the number of participants required to draw futility or efficacy conclusions. We focus on time-to-event and ordinal outcomes. When more than a few baseline covariates are available, a key question for covariate adjustment in randomised studies is how to fit a model relating the outcome and the baseline covariates to maximise precision. We present a novel theoretical result establishing conditions for asymptotic normality of a variety of covariate-adjusted estimators that rely on machine learning (e.g., (Formula presented.) -regularisation, Random Forests, XGBoost, and Multivariate Adaptive Regression Splines [MARS]), under the assumption that outcome data are missing completely at random. We further present a consistent estimator of the asymptotic variance. Importantly, the conditions do not require the machine learning methods to converge to the true outcome distribution conditional on baseline variables, as long as they converge to some (possibly incorrect) limit. We conducted a simulation study to evaluate the performance of the aforementioned prediction methods in COVID-19 trials. Our simulation is based on resampling longitudinal data from over 1500 patients hospitalised with COVID-19 at Weill Cornell Medicine New York Presbyterian Hospital. We found that using (Formula presented.) -regularisation led to estimators and corresponding hypothesis tests that control type 1 error and are more precise than an unadjusted estimator across all sample sizes tested. We also show that when covariates are not prognostic of the outcome, (Formula presented.) -regularisation remains as precise as the unadjusted estimator, even at small sample sizes ((Formula presented.)). We give an R package adjrct that performs model-robust covariate adjustment for ordinal and time-to-event outcomes.},
   author = {Nicholas Williams and Michael Rosenblum and Iván Díaz},
   doi = {10.1111/rssa.12915},
   issn = {1467985X},
   issue = {4},
   journal = {Journal of the Royal Statistical Society. Series A: Statistics in Society},
   title = {Optimising precision and power by machine learning in randomised trials with ordinal and time-to-event outcomes with an application to COVID-19},
   volume = {185},
   year = {2022},
}

@article{Zhao2022,
   abstract = {Randomized experiments allow for consistent estimation of the average treatment effect based on the difference in mean outcomes without strong modeling assumptions. Appropriate use of pretreatment covariates can further improve the estimation efficiency. Missingness in covariates is nevertheless common in practice, and raises an important question: should we adjust for covariates subject to missingness, and if so, how? The unadjusted difference in means is always unbiased. The complete-covariate analysis adjusts for all completely observed covariates, and is asymptotically more efficient than the difference in means if at least one completely observed covariate is predictive of the outcome. Then what is the additional gain of adjusting for covariates subject to missingness? To reconcile the conflicting recommendations in the literature, we analyze and compare five strategies for handling missing covariates in randomized experiments under the design-based framework, and recommend the missingness-indicator method, as a known but not so popular strategy in the literature, due to its multiple advantages. First, it removes the dependence of the regression-adjusted estimators on the imputed values for the missing covariates. Second, it does not require modeling the missingness mechanism, and yields consistent estimators even when the missingness mechanism is related to the missing covariates and unobservable potential outcomes. Third, it ensures large-sample efficiency over the complete-covariate analysis and the analysis based on only the imputed covariates. Lastly, it is easy to implement via least squares. We also propose modifications to it based on asymptotic and finite sample considerations. Importantly, our theory views randomization as the basis for inference, and does not impose any modeling assumptions on the data-generating process or missingness mechanism. Supplementary materials for this article are available online.},
   author = {Anqi Zhao and Peng Ding},
   doi = {10.1080/01621459.2022.2123814},
   issn = {1537274X},
   journal = {Journal of the American Statistical Association},
   title = {To Adjust or not to Adjust? Estimating the Average Treatment Effect in Randomized Experiments with Missing Covariates},
   year = {2022},
}

@article{Li2023,
   abstract = {We present a framework for using existing external data to identify and estimate the relative efficiency of a covariate-adjusted estimator compared to an unadjusted estimator in a future randomized trial. Under conditions, these relative efficiencies approximate the ratio of sample sizes needed to achieve a desired power. We develop semiparametrically efficient estimators of the relative efficiencies for several treatment effect estimands of interest with either fully or partially observed outcomes, allowing for the application of flexible statistical learning tools to estimate the nuisance functions. We propose an analytic Wald-type confidence interval and a double bootstrap scheme for statistical inference. We demonstrate the performance of the proposed methods through simulation studies and apply these methods to estimate the efficiency gain of covariate adjustment in Covid-19 therapeutic trials.},
   author = {Xiudi Li and Sijia Li and Alex Luedtke},
   doi = {10.1093/jrsssb/qkad007},
   issn = {14679868},
   issue = {2},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   title = {Estimating the efficiency gain of covariate-adjusted analyses in future clinical trials using external data},
   volume = {85},
   year = {2023},
}

@article{Pirondini2022,
   abstract = {In randomized controlled trials, patient characteristics are expected to be well balanced between treatment groups; however, adjustment for characteristics that are prognostic can still be beneficial with a modest gain in statistical power. Nevertheless, previous reviews show that many trials use unadjusted analyses. In this article, we review current practice regarding covariate adjustment in cardiovascular trials among all 84 randomized controlled trials relating to cardiovascular disease published in the New England Journal of Medicine, The Lancet, and the Journal of the American Medical Association during 2019. We identify trials in which use of covariate adjustment led to a change in the trial conclusions. By using these trials as case studies, along with data from the CHARM trial and simulation studies, we demonstrate some of the potential benefits and pitfalls of covariate adjustment. We discuss some of the complexities of using covariate adjustment, including how many covariates to choose, how covariates should be modeled, how to handle missing data for baseline covariates, and how adjusted analyses are viewed by regulators. We conclude that contemporary cardiovascular trials do not make best use of covariate adjustment and that more frequent use could lead to improvements in the efficiency of future trials.},
   author = {Leah Pirondini and John Gregson and Ruth Owen and Tim Collier and Stuart Pocock},
   doi = {10.1016/j.jchf.2022.02.007},
   issn = {22131779},
   issue = {5},
   journal = {JACC: Heart Failure},
   title = {Covariate Adjustment in Cardiovascular Randomized Controlled Trials: Its Value, Current Practice, and Need for Improvement},
   volume = {10},
   year = {2022},
}

@article{Kang2007,
abstract = {},
author = {Kang, Joseph D.Y. and Schafer, Joseph L.},
doi = {10.1214/07-STS227},
issn = {08834237},
journal = {Statistical Science},
keywords = {Causal inference,Missing data,Model-assisted survey estimation,Propensity score,Weighted estimating equations},
number = {4},
pages = {523--539},
pmid = {18516239},
title = {{Demystifying double robustness: A comparison of alternative strategies for estimating a population mean from incomplete data}},
volume = {22},
year = {2007}
}


@Manual{R-base,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2022},
  url = {https://www.R-project.org/},
}

@Manual{tidymodels2020,
    title = {Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles.},
    author = {Max Kuhn and Hadley Wickham},
    url = {https://www.tidymodels.org},
    year = {2020},
  }
  
  @Manual{parsnip,
    title = {parsnip: A Common API to Modeling and Analysis Functions},
    author = {Max Kuhn and Davis Vaughan},
    year = {2022},
    note = {R package version 1.0.3},
    url = {https://CRAN.R-project.org/package=parsnip},
  }

@Misc{dbarts2023,
    author = {Vincent Dorie},
    title = {dbarts: Discrete Bayesian Additive Regression Trees Sampler},
    year = {2023},
    note = {R package version 0.9-23},
    url = {https://CRAN.R-project.org/package=dbarts},
  }
  
  @article{Carnegie2019,
   abstract = {},
   author = {Nicole Bohme Carnegie and James Wu},
   doi = {10.1177/2378023119825886},
   issn = {23780231},
   journal = {Socius},
   title = {Variable Selection and Parameter Tuning for BART Modeling in the Fragile Families Challenge},
   volume = {5},
   year = {2019},
}

@article{Hill2020,
   abstract = {},
   author = {Jennifer Hill and Antonio Linero and Jared Murray},
   doi = {10.1146/annurev-statistics-031219-041110},
   issn = {2326831X},
   journal = {Annual Review of Statistics and Its Application},
   title = {Bayesian additive regression trees: A review and look forward},
   volume = {7},
   year = {2020},
}

@article{Chipman2010,
   abstract = {},
   author = {Hugh A. Chipman and Edward I. George and Robert E. McCulloch},
   doi = {10.1214/09-AOAS285},
   issn = {19326157},
   issue = {1},
   journal = {Annals of Applied Statistics},
   title = {BART: Bayesian additive regression trees},
   volume = {6},
   year = {2010},
}

@article{Morris2019,
   abstract = {},
   author = {Tim P. Morris and Ian R. White and Michael J. Crowther},
   doi = {10.1002/sim.8086},
   issn = {10970258},
   issue = {11},
   journal = {Statistics in Medicine},
   title = {Using simulation studies to evaluate statistical methods},
   volume = {38},
   year = {2019},
}

@Manual{BayesTree2016,
    title = {BayesTree: Bayesian Additive Regression Trees},
    author = {Hugh Chipman and Robert McCulloch},
    year = {2016},
    note = {R package version 0.3-1.4},
    url = {https://CRAN.R-project.org/package=BayesTree},
  }
  
  
@article{Lee2021,
   abstract = {Background: Episodes of acute diarrhea lead to dehydration, and existing care algorithms base treatment around categorical estimates for fluid resuscitation. This study aims to develop models for the percentage dehydration (fluid deficit) in individuals with acute diarrhea, to better target treatment and avoid the potential sequelae of over or under resuscitation. Methods: This study utilizes data from two prospective cohort studies of patients with acute diarrhea in Dhaka, Bangladesh. Data were collected on patient arrival, including weight, clinical signs and symptoms, and demographic information. Consecutive weights were obtained to determine the true volume deficit of each patient. Data were entered into two distinct forward stepwise regression logistic models (DHAKA for under 5 years and NIRUDAK for 5 years and over). Results: A total of 782 patients were included in the final analysis of the DHAKA data set, and 2139 were included in the final analysis of the NIRUDAK data set. The best model for the DHAKA data achieved an R2 of 0.27 and a root mean square error (RMSE) of 3.7 (compared to R2 of 0.06 and RMSE of 5.5 with the World Health Organization child care algorithm) and selected 6 predictors. The best performance model for the NIRUDAK data achieved an R2 of 0.28 and a RMSE of 2.6 (compared to R2 of 0.08 and RMSE of 4.3 with the World Health Organization adolescent/adult care algorithm) and selected 7 predictors with 2 interactions. Conclusions: These are the first mathematical models for patients with acute diarrhea that allow for the calculation of a patient’s percentage dehydration (fluid deficit) and subsequent targeted treatment with fluid resuscitation. These findings are an improvement on existing World Health Organization care algorithms.},
   author = {J. Austin Lee and Kexin Qu and Monique Gainey and Samika S. Kanekar and Meagan A. Barry and Sabiha Nasrin and Nur H. Alam and Christopher H. Schmid and Adam C. Levine},
   doi = {10.1186/s41182-021-00361-9},
   issn = {13494147},
   issue = {1},
   journal = {Tropical Medicine and Health},
   title = {Continuous diagnostic models for volume deficit in patients with acute diarrhea},
   volume = {49},
   year = {2021},
}

